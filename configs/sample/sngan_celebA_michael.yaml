fid_b_size: 100
b_size : 2000
dataset: celebA

total_epochs: 100

g_model : 'sngan'
d_model : 'sngan'

optimizer: Adam

criterion : 'kale'

# load pretrained weights
# use a single generator for all tests
#d_path : '/nfs/gatsbystor/michaela/projects/kale/exp/rebuttal/sngan_celebA/train/checkpoints_7148478/d_47768.pth'
#g_path : '/nfs/gatsbystor/michaela/projects/kale/exp/rebuttal/sngan_celebA/train/checkpoints_7148478/g_47768.pth'

#d_path : '/nfs/gatsbystor/michaela/projects/kale/exp/rebuttal/sngan_celebA/train/checkpoints_7148478/d_85928.pth'
g_path : '/nfs/gatsbystor/michaela/projects/kale/exp/rebuttal/sngan_celebA/train/checkpoints_7148478/g_85928.pth'



d_path : '/nfs/gatsbystor/michaela/projects/kale/exp/rebuttal/sampling_celebA_4/tune_and_fids/checkpoints_1957159/d_best.pth'


log_dir : '/nfs/gatsbystor/michaela/projects/kale/exp/neurips'
data_path : '/nfs/gatsbystor/michaela/projects/data/'

log_name : 'sampling_celebA_5'

penalty_type : 'gradient'
penalty_lambda : 1.

scheduler : 'ExponentialLR'
scheduler_gamma : 0.8
beta_1 : 0.5
beta_2 : 0.999

lr : 0.00001

with_fid : True

# train
mode : sample
fid_samples : 50000

# sampling stuff
Z_dim : 100
sample_b_size : 2000




lmc_gamma : .00001
lmc_kappa : .04
#latent_sampler : 'langevin'
num_lmc_steps : 1000
seed : 0
eval_fid : True
log_to_file : True



#temperature : 100.


